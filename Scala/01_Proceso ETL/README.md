# Introducci√≥n a Apache Spark con Scala

## üåü ¬øQu√© es Apache Spark?
Apache Spark es un motor de procesamiento de datos en cl√∫ster de c√≥digo abierto dise√±ado para velocidad y facilidad de uso. Permite el procesamiento de grandes vol√∫menes de datos de manera distribuida y eficiente, soportando m√∫ltiples lenguajes como Scala, Python, Java y R.

Scala es el lenguaje nativo de Spark y ofrece ventajas de rendimiento al ejecutarse en la JVM.

## üõ†Ô∏è Uso de Scala en Databricks
Databricks es una plataforma optimizada para trabajar con Apache Spark. Para usar Scala en Databricks, sigue estos pasos:

1. **Crear un Notebook:**
   - Entra a tu cuenta de Databricks.
   - En la barra lateral izquierda, selecciona "Workspace" y crea un nuevo notebook.
   - Asigna un nombre y elige **Scala** como lenguaje de programaci√≥n.

2. **Configurar el Cluster:**
   - En la pesta√±a "Clusters", inicia o crea un nuevo cluster.
   - Aseg√∫rte de que el cluster tiene una versi√≥n compatible de Apache Spark.
   - Asigna el notebook al cluster para poder ejecutar c√≥digo Scala.

## üìù Comandos B√°sicos en Spark con Scala

### üîß Declaraci√≥n de Variables en Scala
En Scala, las variables se pueden declarar de dos maneras:

- **`var`**: Permite modificar su valor despu√©s de la asignaci√≥n.
- **`val`**: Crea una constante inmutable.

Ejemplo:
```scala
var nombre = "Apache Spark"
nombre = "Big Data" // Permitido

val version = "3.5.0"
// version = "3.4.0" // Error: no se puede reasignar un val
```

### üõ†Ô∏è Creaci√≥n de una Sesi√≥n de Spark
Para trabajar con Spark, se debe inicializar una sesi√≥n:
```scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("Ejemplo Spark")
  .master("local[*]")
  .getOrCreate()
```
En Databricks, la sesi√≥n de Spark ya est√° creada, por lo que puedes usar `spark` directamente.

## üìö RDDs, DataFrames y Datasets en Spark

Spark ofrece tres estructuras principales para manejar datos: **RDDs, DataFrames y Datasets**.

### üìä RDD (Resilient Distributed Dataset)
Los RDDs son la estructura m√°s b√°sica y flexible en Spark. Son inmutables, tolerantes a fallos y distribuidos en un cl√∫ster.
```scala
val rdd = spark.sparkContext.parallelize(Seq((1, "Juan"), (2, "Mar√≠a")))
rdd.collect().foreach(println)
```

### üìä DataFrames
Los DataFrames son estructuras tabulares optimizadas, similares a las tablas SQL, con un mejor rendimiento que los RDDs.
```scala
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

val data = Seq((1, "Juan", 25), (2, "Mar√≠a", 30))
val df: DataFrame = spark.createDataFrame(data).toDF("id", "nombre", "edad")
df.show()
```

### üìä Datasets
Los Datasets combinan la eficiencia de los DataFrames con la seguridad de tipos de datos de los RDDs. Se usan principalmente con Scala y Java.
```scala
case class Persona(id: Int, nombre: String, edad: Int)
val ds = df.as[Persona]
ds.show()
```

**Diferencias clave:**
| Estructura | Seguridad de tipos | Optimizado | Uso recomendado |
|------------|-------------------|------------|----------------|
| RDD        | ‚úÖ Alta          | ‚ùå No      | Transformaciones complejas |
| DataFrame  | ‚ùå Baja          | ‚úÖ S√≠      | Grandes vol√∫menes de datos |
| Dataset    | ‚úÖ Alta          | ‚úÖ S√≠      | Seguridad de tipos en Scala |

## üìÇ Lectura y Escritura de Datos
```scala
val df = spark.read.option("header", "true").csv("/mnt/datos/datos.csv")
df.write.format("parquet").save("/mnt/datos/output.parquet")
```

## üëÅÔ∏è Transformaciones y Acciones en Spark
### Transformaciones (Lazy Evaluation)
Las transformaciones no se ejecutan inmediatamente, sino que crean un plan de ejecuci√≥n que se materializa solo cuando se requiere un resultado.
```scala
val dfFiltrado = df.filter(col("edad") > 25)
val dfSeleccionado = dfFiltrado.select("nombre", "edad")
```

### Acciones (Ejecutan el DAG)
Las acciones disparan la ejecuci√≥n de las transformaciones y devuelven un resultado.
```scala
df.show() // Muestra los datos
df.count() // Cuenta las filas
df.collect() // Devuelve un array con los datos
```

## üî¢ Operaciones sobre DataFrames
### üë§ Agrupamiento de Datos
```scala
df.groupBy("edad").agg(count("id"), sum("edad"), max("edad")).show()
```

### üåç Lectura de JSON con Esquema
```scala
import org.apache.spark.sql.types._
val esquema = StructType(Array(
  StructField("id", IntegerType, true),
  StructField("nombre", StringType, true),
  StructField("edad", IntegerType, true)
))
val dfJson = spark.read.schema(esquema).json("/mnt/datos/datos.json")
dfJson.show()
```

### ‚ûï Agregar Columnas
```scala
val dfNuevo = df.withColumn("edad_5_anios_despues", col("edad") + 5)
dfNuevo.show()
```

### ü§ù Combinaciones de DataFrames (Join)
```scala
val df1 = Seq((1, "Juan"), (2, "Mar√≠a")).toDF("id", "nombre")
val df2 = Seq((1, "Lima"), (2, "Cusco")).toDF("id", "ciudad")
val dfJoin = df1.join(df2, "id")
dfJoin.show()
```

## üí° Conclusi√≥n
Apache Spark con Scala es una poderosa combinaci√≥n para el procesamiento distribuido de datos. Scala permite escribir c√≥digo eficiente y expresivo para manipular grandes vol√∫menes de datos en Spark.

Para aprender m√°s, visita la documentaci√≥n oficial de Spark: [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/)
